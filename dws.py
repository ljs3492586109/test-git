from pyspark.sql import SparkSession
import sys
from pyspark.sql.functions import lit

# ç›®æ ‡æ—¥æœŸ
TARGET_DT = "2025-07-10"


def get_spark_session():
    """åˆ›å»ºè¿æ¥åˆ°CDH01 Hive Metastoreçš„Sparkä¼šè¯"""
    try:
        spark = SparkSession.builder \
            .appName("DWS_Data_Duplicator") \
            .enableHiveSupport() \
            .config("hive.metastore.uris", "thrift://cdh01:9083") \
            .config("spark.driver.memory", "4g") \
            .config("spark.executor.memory", "8g") \
            .config("spark.sql.debug.maxToStringFields", "1000") \
            .getOrCreate()
        print("âœ… Sparkä¼šè¯å·²æˆåŠŸåˆ›å»ºï¼Œå·²è¿æ¥åˆ°CDH01çš„Hive")
        return spark
    except Exception as e:
        print(f"âŒ åˆ›å»ºSparkä¼šè¯å¤±è´¥: {str(e)}")
        sys.exit(1)


def list_dws_tables(spark):
    """åˆ—å‡ºdwsåº“ä¸­çš„æ‰€æœ‰è¡¨"""
    try:
        spark.sql("USE dws")
        tables_df = spark.sql("SHOW TABLES")
        tables = [row.tableName for row in tables_df.collect()]
        print(f"\nğŸ“Š dwså±‚å…±å‘ç° {len(tables)} å¼ è¡¨:")
        for i, table in enumerate(tables, 1):
            print(f"  {i}. {table}")
        return tables
    except Exception as e:
        print(f"âŒ è·å–dwså±‚è¡¨åˆ—è¡¨å¤±è´¥: {str(e)}")
        return []


def copy_table_with_new_dt(spark, table_name):
    """å¤åˆ¶è¡¨æ•°æ®å¹¶å°†dtå­—æ®µä¿®æ”¹ä¸ºç›®æ ‡æ—¥æœŸï¼Œä½¿ç”¨Hive SQLæ’å…¥"""
    try:
        print(f"\n===== å¤„ç†è¡¨: dws.{table_name} =====")

        # 1. æ£€æŸ¥æ˜¯å¦æœ‰dtå­—æ®µ
        df_original = spark.table(f"dws.{table_name}")
        if "dt" not in df_original.columns:
            print(f"âš ï¸ è¡¨ {table_name} ä¸åŒ…å«dtå­—æ®µï¼Œè·³è¿‡å¤„ç†")
            return "skipped"

        # 2. è·å–åŸå§‹æ•°æ®é‡
        original_count = df_original.count()
        print(f"ğŸ“¥ åŸå§‹æ•°æ®é‡: {original_count} è¡Œ")

        # 3. æ„å»ºä¸åŒ…å«dtçš„å­—æ®µåˆ—è¡¨ï¼ˆè§£å†³åˆ—æ•°ä¸åŒ¹é…é—®é¢˜ï¼‰
        non_partition_columns = [col for col in df_original.columns if col != 'dt']
        columns_str = ', '.join(non_partition_columns)

        # 4. ä½¿ç”¨Hive SQLæ’å…¥æ•°æ®ï¼ˆåˆ†åŒºå­—æ®µåœ¨PARTITIONä¸­æŒ‡å®šï¼ŒSELECTä¸åŒ…å«dtï¼‰
        print(f"\nğŸš§ æ­£åœ¨é€šè¿‡Hive SQLæ’å…¥æ•°æ®åˆ°dt={TARGET_DT} åˆ†åŒº...")
        insert_query = f"""
            INSERT INTO TABLE dws.{table_name} PARTITION (dt='{TARGET_DT}')
            SELECT {columns_str}
            FROM dws.{table_name}
        """
        spark.sql(insert_query)

        # 5. éªŒè¯å†™å…¥ç»“æœ
        new_partition_df = spark.table(f"dws.{table_name}").where(f"dt = '{TARGET_DT}'")
        new_partition_count = new_partition_df.count()

        print(f"\nâœ… å¤åˆ¶å®Œæˆï¼")
        print(f"   æ–°åˆ†åŒº dt={TARGET_DT} çš„æ•°æ®é‡: {new_partition_count} è¡Œ")

        if new_partition_count == original_count:
            print("   âœ… æ–°åˆ†åŒºæ•°æ®é‡ä¸åŸå§‹æ•°æ®é‡ä¸€è‡´")
            return "success"
        else:
            print(f"   âš ï¸ æ–°åˆ†åŒºæ•°æ®é‡ä¸åŸå§‹æ•°æ®é‡ä¸ä¸€è‡´ï¼ˆåŸå§‹: {original_count}, æ–°åˆ†åŒº: {new_partition_count}ï¼‰")
            return "mismatch"

    except Exception as e:
        print(f"âŒ å¤„ç†è¡¨ {table_name} å¤±è´¥: {str(e)}")
        return "failed"


def main():
    # 1. åˆ›å»ºSparkä¼šè¯
    spark = get_spark_session()
    print(f"ğŸ¯ ç›®æ ‡æ—¥æœŸ: dt={TARGET_DT}")

    # 2. è·å–dwså±‚è¡¨åˆ—è¡¨
    tables = list_dws_tables(spark)
    if not tables:
        spark.stop()
        return

    # 3. æ‰¹é‡å¤„ç†æ‰€æœ‰è¡¨
    processed = 0
    skipped = 0
    failed = 0
    mismatch = 0

    for i, table in enumerate(tables, 1):
        print(f"\n({i}/{len(tables)}) æ­£åœ¨å¤„ç†è¡¨: {table}")
        result = copy_table_with_new_dt(spark, table)
        if result == "success":
            processed += 1
        elif result == "skipped":
            skipped += 1
        elif result == "failed":
            failed += 1
        elif result == "mismatch":
            processed += 1
            mismatch += 1

    # 4. è¾“å‡ºå¤„ç†ç»“æœ
    print(f"\n===== å¤„ç†å®Œæˆ =====")
    print(f"  æˆåŠŸå¤„ç†: {processed} å¼ è¡¨")
    print(f"  æ•°æ®é‡ä¸åŒ¹é…: {mismatch} å¼ è¡¨")
    print(f"  è·³è¿‡: {skipped} å¼ è¡¨ï¼ˆæ— dtå­—æ®µï¼‰")
    print(f"  å¤±è´¥: {failed} å¼ è¡¨")

    # 5. å…³é—­ä¼šè¯
    spark.stop()
    print("\nâœ… Sparkä¼šè¯å·²å…³é—­")


if __name__ == "__main__":
    main()